{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71ce9c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "path_list_train_cv = \"allcat_train_cv_json_list_inside.txt\"\n",
    "path_list_test = \"allcat_test_json_list_inside.txt\"\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56584fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_from_json_object(json_object):\n",
    "    text = \"\"\n",
    "    for i in json_object[\"paragraphs\"]:\n",
    "        text+=i[\"text\"]+\" \"#appending a space at end so that there is a gap when next sentence is added\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bba7afb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "    for removing special characters, lowering the case and removing \n",
    "    punctuations\n",
    "    \n",
    "    params:\n",
    "        text: str - text string of the document to be cleaned \n",
    "    returns:\n",
    "        text_list : list of cleaned text that has length one. \n",
    "                    tfidf_vectoriser accepts only list of strings\n",
    "    '''\n",
    "    pattern = r'[^A-Za-z ]+'\n",
    "    clean_text = [re.sub(pattern,'', str(x.lower())) for x in text.split(\" \")]\n",
    "#     text_list = [\" \".join(clean_text)]\n",
    "    text_list = \" \".join(clean_text)\n",
    "    #if you are sending this to the model,\n",
    "    #send it as a list, Since Model takes list of text\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58b70808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_headings_and_para(file):\n",
    "    '''\n",
    "    file: json object\n",
    "    It checks all paragraphs and filters those will length less than 5\n",
    "    '''\n",
    "    n_headings = 0\n",
    "    n_paragraphs = 0\n",
    "    headings = []\n",
    "    for para in file[\"paragraphs\"]:\n",
    "        if para[\"is_valid\"]:\n",
    "            if len(para[\"text\"]) > 0:\n",
    "                if len(para[\"text\"].split(\" \")) < 5:\n",
    "                    n_headings+=1\n",
    "                    clean_heading = clean_text(para[\"text\"])\n",
    "                    headings.append(clean_heading)\n",
    "                else:\n",
    "                    count=0\n",
    "                    for run in para[\"runs\"]:\n",
    "                        count+=1\n",
    "                        if (run[\"all_caps\"] or run[\"bold\"]) and len(run[\"text\"]) > 6:\n",
    "                            clean_heading = clean_text(run[\"text\"])\n",
    "                            headings.append(clean_heading)\n",
    "                        if count == 2:#checking only for first two runs. As that's where headings are more probable\n",
    "                            break \n",
    "                            \n",
    "    return n_headings, headings, n_paragraphs\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78c55e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm data_json_space_corrected_with_tables_try_inline_headings.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16b53a20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2792it [01:25, 32.68it/s] \n"
     ]
    }
   ],
   "source": [
    "f_result = open(\"data_json_space_corrected_with_tables_try_inline_headings.csv\", \"a+\")\n",
    "f_result.write('doc_name,doc_category,domain, status,word_count,n_tables,n_paragraphs,n_images,n_headings,heading_text,text\\n')\n",
    "with open(\"raw_json_list.txt\") as f:\n",
    "    for i in tqdm(f):\n",
    "        path= i.rstrip(\"\\n\")\n",
    "#         print(path)\n",
    "#         path = \"./Journal_Articles/Physical Sciences/Original/ADDMH_2_input.json\"\n",
    "        _,doc_category,domain,status,name = path.split(\"/\")\n",
    "#         print(doc_category,domain,status,name)\n",
    "        file = json.load(open(path))\n",
    "#         print(file.keys())\n",
    "#         print(file[\"paragraphs\"][0].keys())\n",
    "        if status == \"Original\" and file[\"status\"]:\n",
    "            n_tables = len(file[\"tables\"])\n",
    "            n_images = file[\"images_count\"]\n",
    "            word_count = file[\"word_count\"]\n",
    "            n_tables = len(file[\"tables\"])\n",
    "            n_headings, headings, n_paragraphs = find_headings_and_para(file)\n",
    "            text = clean_text(load_text_from_json_object(file))\n",
    "            heading_text = \" \".join(headings)\n",
    "            result = f'{name},{doc_category},{domain},{status},{word_count},{n_tables},{n_paragraphs},{n_images},{n_headings},{heading_text},{text}\\n'\n",
    "            f_result.write(result)\n",
    "#             print(text)\n",
    "f_result.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
